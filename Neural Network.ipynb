{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementasi Oleh Kelompok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class NeuralNet:\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, nodes_n_in_hidden_layers, learning_rate, momentum):\n",
    "        nodes_n_in_hidden_layers.append(1) # satu node buat output\n",
    "        self.nodes_n_in_hidden_layers = nodes_n_in_hidden_layers\n",
    "        self.inputs = [] #data input\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.outputs = [] # output dari setiap node pada satu iterasi\n",
    "        self.weights = [] # weight dari setiap edge pada satu iterasi\n",
    "        self.biases = [] # bias dari setiap node pada satu iterasi\n",
    "        self.weight_biases = []\n",
    "        self.local_gradients = [] # local gradient dari setiap node pada satu iterasi\n",
    "        self.delta_weights = [] # delta weight dari setiap edge pada satu iterasi\n",
    "        self.delta_biases = [] # delta bias dari setiap node pada satu iterasi\n",
    "        self.layer_nodes = [] # node-node pada layer-layer\n",
    "        self.v = [] # v pada setiap node\n",
    "        self.targets = [] # target dari data input\n",
    "        self.test_outputs = [] # hasil predict data test\n",
    "        self.test_labels = [] # label hasil predict data test\n",
    "        \n",
    "    # Feed Forward\n",
    "    def feed_forward(self, datum_idx):\n",
    "        for i in range (1, len(self.layer_nodes)):\n",
    "            for j in range (0, len(self.layer_nodes[i])):\n",
    "                current_node = self.layer_nodes[i][j]\n",
    "                weights = []\n",
    "                weights.append(self.biases[current_node])\n",
    "                inputs = []\n",
    "                inputs.append(1)\n",
    "                for k in range(0, len(self.layer_nodes[i-1])):\n",
    "                    if (self.weights[self.layer_nodes[i-1][k]][current_node] != None):\n",
    "                            weights.append(self.weights[self.layer_nodes[i-1][k]][current_node])\n",
    "                    \n",
    "                    if (i==1):\n",
    "                        inputs.append(self.inputs[datum_idx][self.layer_nodes[i-1][k]])\n",
    "                    else:\n",
    "                        inputs.append(self.outputs[self.layer_nodes[i-1][k]])\n",
    "                v = np.dot(inputs, weights)\n",
    "                self.v[current_node] = v\n",
    "                self.outputs[current_node] = sigmoid(v)\n",
    "                \n",
    "    # Back Propagation\n",
    "    def back_propagation(self, datum_idx):\n",
    "        for i in range(len(self.layer_nodes)-1, 0, -1):\n",
    "            for j in range(len(self.layer_nodes[i])-1, -1, -1):\n",
    "                current_node = self.layer_nodes[i][j]\n",
    "                if (i == len(self.layer_nodes)-1):\n",
    "                    v = self.v[current_node]\n",
    "                    sig_v = sigmoid(v)\n",
    "                    self.local_gradients[current_node] = self.local_gradients[current_node] + (sig_v * (1 - sig_v) * (self.targets[datum_idx] - self.outputs[current_node]))\n",
    "                else:\n",
    "                    v = self.v[current_node]\n",
    "                    sig_v = sigmoid(v)\n",
    "                    weight_delta = 1\n",
    "                    for k in range(0, len(self.layer_nodes[i+1])):\n",
    "                        weight_delta = weight_delta * self.local_gradients[self.layer_nodes[i+1][k]] * self.weights[current_node][self.layer_nodes[i+1][k]]\n",
    "                    self.local_gradients[current_node] = self.local_gradients[current_node] + (sig_v * (1 - sig_v) * weight_delta)            \n",
    "                \n",
    "    # Update Weight\n",
    "    def update_weight(self):\n",
    "        for i in range(0, len(self.layer_nodes)-1):\n",
    "            for j in range(0, len(self.layer_nodes[i])):\n",
    "                current_node = self.layer_nodes[i][j]\n",
    "                for k in range(0, len(self.layer_nodes[i+1])):\n",
    "                    current_next_node = self.layer_nodes[i+1][k]\n",
    "                    self.delta_weights[current_node][current_next_node] = self.momentum * self.delta_weights[current_node][current_next_node] + self.learning_rate * self.local_gradients[current_next_node] * self.outputs[current_next_node]\n",
    "                    self.weights[current_node][current_next_node] += self.delta_weights[current_node][current_next_node]\n",
    "        for i in range(1, len(self.biases)):\n",
    "            new_bias = self.biases[i] + self.momentum  * self.delta_biases[i] + self.learning_rate * self.local_gradients[i]\n",
    "            self.delta_biases[i] = new_bias - self.biases[i]\n",
    "            self.biases[i] = new_bias\n",
    "            \n",
    "    # Fit\n",
    "    def fit(self, X, Y, batch_size, max_iter, threshold): # data = array of arrays\n",
    "        #data[0] ke n merupakan label\n",
    "        #nodes_n_in_hidden_layers[0] merupakan jumlah input\n",
    "        self.nodes_n_in_hidden_layers.insert(0, len(X[0])) \n",
    "        \n",
    "        self.targets = Y\n",
    "        self.inputs = X\n",
    "        \n",
    "        n_nodes = 0\n",
    "        init_weight = 1 # Weights diinisalisasi 0\n",
    "        \n",
    "        # Inisialisasi output, bias, local gradient, v, dan delta bias di setiap node pada layer\n",
    "        for i in range(0, len(self.nodes_n_in_hidden_layers)):\n",
    "            l_nodes = []\n",
    "            for j in range(0, self.nodes_n_in_hidden_layers[i]):\n",
    "                self.outputs.append(0)\n",
    "                self.v.append(0)\n",
    "                self.biases.append(0) #asumsi x bias = 1\n",
    "                self.local_gradients.append(0)\n",
    "                self.delta_biases.append(0)\n",
    "                l_nodes.append(n_nodes)\n",
    "                n_nodes += 1\n",
    "            self.layer_nodes.append(l_nodes)\n",
    "        \n",
    "        for i in range(0, n_nodes):\n",
    "            self.weights.append([])\n",
    "            self.delta_weights.append([])\n",
    "            for j in range(0, n_nodes):\n",
    "                self.weights[i].append(None)\n",
    "                self.delta_weights[i].append(None)\n",
    "            \n",
    "        current_node = 0\n",
    "        for i in range(0, len(self.nodes_n_in_hidden_layers)-1):\n",
    "            if (i < len(self.nodes_n_in_hidden_layers)-1):\n",
    "                next_layer_first_node = current_node + self.nodes_n_in_hidden_layers[i]\n",
    "                for j in range(0, self.nodes_n_in_hidden_layers[i]):\n",
    "                    for k in range(0, self.nodes_n_in_hidden_layers[i+1]):\n",
    "                        self.weights[current_node][k+next_layer_first_node] = random.uniform(-0.5,0.5)\n",
    "                        self.delta_weights[current_node][k+next_layer_first_node] = 0\n",
    "                    current_node += 1\n",
    "        \n",
    "        n_batch = math.ceil(len(Y)/batch_size)\n",
    "        \n",
    "        n_iter = 0\n",
    "        error = 100\n",
    "        while (n_iter < max_iter and error > threshold):\n",
    "            datum_idx = 0\n",
    "            for i in range(0, n_batch):\n",
    "                # Mengembalikan local_gradient menjadi 0\n",
    "                for i in range(0, n_nodes):\n",
    "                    self.local_gradients[i] = 0\n",
    "                    \n",
    "                j = 0\n",
    "                accum_error = 0\n",
    "                num_datum = 0\n",
    "                while (j < batch_size):\n",
    "                    if (datum_idx < len(Y)):\n",
    "                        self.feed_forward(datum_idx)\n",
    "                        accum_error = accum_error + (0.5 * ((self.targets[datum_idx] - self.outputs[-1])**2))\n",
    "                        num_datum = num_datum + 1\n",
    "                        self.back_propagation(datum_idx)\n",
    "                        datum_idx += 1\n",
    "                        j += 1\n",
    "                    else:\n",
    "                        j = batch_size + 1\n",
    "                error = accum_error / num_datum\n",
    "                if (error < threshold):\n",
    "                    break\n",
    "                self.update_weight()\n",
    "            \n",
    "            n_iter += 1\n",
    "        \n",
    "    # Predict\n",
    "    def predict(self, data_test):\n",
    "        feed_forward_result = []\n",
    "        self.inputs = data_test\n",
    "        \n",
    "        for i in range (0, len(data_test)):\n",
    "            self.feed_forward(i)\n",
    "            self.test_outputs.append(self.outputs[-1])\n",
    "        \n",
    "        return self.test_outputs\n",
    "    \n",
    "    def predict_label(self):\n",
    "        for i in range (0, len(self.test_outputs)):\n",
    "            if self.test_outputs[i] >= 0.5:\n",
    "                self.test_labels.append(1)\n",
    "            else:\n",
    "                self.test_labels.append(0)\n",
    "        return self.test_labels\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Penjelasan Model Neural Network\n",
    "Model Neural Network pada program kami diimplementasikan dengan mendefinisikan variabel-variabel (inputs, outputs, jumlah node pada satu layer, dll) dalam bentuk matrix dan list. Pada saat penciptaan suatu obyek Neural Network, diperlukan parameter jumlah node di setiap layer, learning rate, dan momentum. Setelah itu, obyek Neural Network yang terbentuk akan menginisiasi beberapa variabel seperti weights, biases, delta weights, delta biases, local gradients, dan beberapa variabel lainnya. \n",
    "\n",
    "Pada kelas Neural Network, terdapat sejumlah fungsi, yaitu: \n",
    "1. Feed forward\n",
    "2. Back propagation\n",
    "3. Update weight\n",
    "4. Fit\n",
    "5. Predict\n",
    "6. Predict label\n",
    "\n",
    "### 1. Feed Forward\n",
    "Pada fungsi ini, program akan melakukan pembelajaran terhadap setiap node dengan memasukkan seluruh weight (termasuk weight bias) dan seluruh nilai (termasuk bias) yang menuju ke node tersebut ke dalam list weight dan list input. Kedua list tersebut kemudian di kalikan secara dot matrix yang kemudian hasil tersebut akan dimasukkan ke dalam fungsi sigmoid untuk mendapatkan satu output dari node tersebut. Hasil output tersebut kemudian dijadikan input untuk menghitung output pada node selanjutnya.\n",
    "\n",
    "### 2. Back Propagation\n",
    "Pada fungsi back propagation, dilakukan perhitungan untuk memperbarui nilai local gradient setiap node. Local gradient dihitung dengan mengalikan turunan dari fungsi sigmoid terhadap v dengan error dari suatu output. Back propagation pada suatu batch dijumlahkan untuk kemudian digunakan sebagai parameter pengubah weight pada fungsi update weight.\n",
    "\n",
    "### 3. Update Weight\n",
    "Pada fungsi update weight, dilakukan perhitungan untuk mendapatkan delta weight dan weight baru. Adapun delta_weight = momentum x delta_weight sebelumnya + learning_rate x local_gradient x output. Perhitungan ini dilakukan bukan hanya pada hubungan antarsimpul, tetapi juga pada bias tiap simpul.\n",
    "\n",
    "### 4. Fit\n",
    "Pada fungsi ini program melakukan pembelajaran terhadap data input. Pembelajaran dilakukan dengan mengkombinasikan ketiga fungsi sebelumnya yaitu feed forward, back propagation, dan update weight yang dieksekusi secara berurutan yang dilakukan sebanyak max_iter atau dilakukan hingga error yang dihasilkan <= threshold.\n",
    "\n",
    "Selain itu pada fungsi ini diimplementasikan juga mini-batch stochastic gradient descent. Implementasi dilakukan dengan cara menerima input berupa batch_size. Program kemudian akan melakukan update weight setelah dilakukan feed forward dan back propagation sebanyak n_batch kali, di mana n_batch adalah jumlah datum dibagi dengan batch_size. Jika nilainya tidak bulat, diambil pembulatan ke atas.\n",
    "\n",
    "### 5. Predict\n",
    "Pada fungsi ini program melakukan prediksi terhadap data test. Prediksi dilakukan dengan cara memanfaatkan model hasil fit dan menjalankan fungsi feed forward untuk setiap data test. \n",
    "\n",
    "### 6. Predict Label\n",
    "Fungsi ini hanya mengembalikan label hasil prediksi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementasi dengan Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers\n",
    "\n",
    "class KerasNeuralNet:\n",
    "    \n",
    "    # Construction\n",
    "    def __init__(self, nodes_n_in_hidden_layers, learning_rate, momentum):\n",
    "        nodes_n_in_hidden_layers.append(1) # satu node buat output\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(output_dim=nodes_n_in_hidden_layers[0], input_dim=4, activation=\"sigmoid\"))\n",
    "        for i in range(1, len(nodes_n_in_hidden_layers)):\n",
    "            self.model.add(Dense(output_dim=nodes_n_in_hidden_layers[i], input_dim=nodes_n_in_hidden_layers[i-1], activation='sigmoid'))\n",
    "        sgd = optimizers.SGD(lr=learning_rate, momentum=momentum, nesterov=False)\n",
    "        self.model.compile(loss='mean_squared_error', optimizer=sgd, metrics=['accuracy'])\n",
    "        \n",
    "    def fit(self, X, Y, batch_size, max_iter):\n",
    "        self.model.fit(X, Y, batch_size=batch_size, epochs=max_iter)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Penjelasan Implementasi dengan Keras\n",
    "Pada kelas KerasNeuralNet, terdapat 3 fungsi yaitu:\n",
    "1. __init__\n",
    "2. fit\n",
    "3. predict\n",
    "\n",
    "### 1. __init__\n",
    "Untuk membuat Neural Network dengan Keras, masukan yang dibutuhkan adalah jumlah simpul pada setiap layer, learning_rate, dan momentum. Pada inisialisasi, Neural Network akan membuat model dengan fungsi Sequential yang sudah tersedia dengan Keras. Layer pertama akan memiliki input_dim berdasarkan jumlah atribut data, sedangkan layer-layer setelahnya akan memiliki input_dim berdasarkan jumlah simpul pada layer sebelumnya. Output_dim = jumlah node pada layer setelahnya. Optimizer yang digunakan adalah Stochastic Gradient Descent dengan lr = learning_rate, momentum=momentum, dan nesterov=False. Nilai nesterov diberikan False agar lebih mirip dengan implementasi kelompok, di mana implementasi kelompok tidak menggunakan nesterov momentum. Dengan alasan yang sama, loss yang digunakan adalah mean_squared_error.\n",
    "\n",
    "### 2. fit\n",
    "Fungsi memanggil fungsi fit yang sudah tersedia untuk objek Sequential pada Keras.\n",
    "\n",
    "### 3. predict\n",
    "Fungsi memanggil fungsi predict yang sudah tersedia untuk objek Sequential pada Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Membaca Data Tennis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suzaneringoringo/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load data\n",
    "dataframe = pd.read_csv('tennis.csv')\n",
    "\n",
    "# Transform outlook, temperature, humidity, and windy to numerical values\n",
    "le = preprocessing.LabelEncoder()\n",
    "encoded = dataframe.apply(le.fit_transform)\n",
    "dataset = encoded.values\n",
    "\n",
    "# X and Y values\n",
    "X = dataset[:,0:4]\n",
    "Y = dataset[:,4]\n",
    "\n",
    "# Rescale min and max for X\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "rescaledX = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting dengan Hasil Implementasi Kelompok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6371934873427952, 0.6336026730531541, 0.6539361621846131, 0.6508008415326048, 0.6450460198663713, 0.6413939157872407, 0.6496933577094223, 0.6424431892105261, 0.6366263075766724, 0.6552220214899362, 0.6432353410188569, 0.6552559471539289, 0.6583535244641116, 0.6470597371112335]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNet([6], 0.25, 0.0001)\n",
    "nn.fit(rescaledX, Y, 1, 5, 0.01)\n",
    "nn.predict(rescaledX)\n",
    "print(nn.test_outputs)\n",
    "print(nn.predict_label())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting dengan Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suzaneringoringo/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=4, activation=\"sigmoid\", units=6)`\n",
      "  del sys.path[0]\n",
      "/home/suzaneringoringo/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=6, activation=\"sigmoid\", units=1)`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 0.2565 - acc: 0.5714\n",
      "Epoch 2/5\n",
      "14/14 [==============================] - 0s 930us/step - loss: 0.2412 - acc: 0.6429\n",
      "Epoch 3/5\n",
      "14/14 [==============================] - 0s 911us/step - loss: 0.2368 - acc: 0.6429\n",
      "Epoch 4/5\n",
      "14/14 [==============================] - 0s 970us/step - loss: 0.2324 - acc: 0.6429\n",
      "Epoch 5/5\n",
      "14/14 [==============================] - 0s 922us/step - loss: 0.2331 - acc: 0.6429\n",
      "[[ 0.63403666]\n",
      " [ 0.59249121]\n",
      " [ 0.68072295]\n",
      " [ 0.66368133]\n",
      " [ 0.66530591]\n",
      " [ 0.62578082]\n",
      " [ 0.64969957]\n",
      " [ 0.64091301]\n",
      " [ 0.64231777]\n",
      " [ 0.67312676]\n",
      " [ 0.61770022]\n",
      " [ 0.64808822]\n",
      " [ 0.69283575]\n",
      " [ 0.62445682]]\n",
      "[[ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]]\n"
     ]
    }
   ],
   "source": [
    "knn = KerasNeuralNet([6], 0.25, 0.0001)\n",
    "\n",
    "# Train model\n",
    "knn.fit(rescaledX, Y, 1, 5)\n",
    "\n",
    "# Predict\n",
    "labels = knn.predict(rescaledX)\n",
    "print(labels)\n",
    "for i in range(0, len(labels)):\n",
    "    if (labels[i] >= 0.5):\n",
    "        labels[i] = 1\n",
    "    else:\n",
    "        labels[i] = 0\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perbandingan Hasil Implementasi Kelompok dengan Keras\n",
    "Meskipun label yang dihasilkan sama persis, nilai sebenarnya dari feed_forward berbeda. Hal ini dapat terjadi karena nilai weights pada saat inisialisasi adalah random sehingga kemungkinan besar berbeda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Pembagian Tugas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Helena Suzane Graciella (13515032)\n",
    "    - Update Weight\n",
    "    - Implementasi Keras\n",
    "    - Fit\n",
    "    - Laporan\n",
    "2. Lathifah Nurrahmah (13515046)\n",
    "    - Back Propagation\n",
    "    - Fit\n",
    "    - Predict\n",
    "    - Laporan\n",
    "3. Aya Aurora Rimbamorani (13515098)\n",
    "    - Feed Forward\n",
    "    - Fit\n",
    "    - Predict & Predict Label\n",
    "    - Laporan"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
