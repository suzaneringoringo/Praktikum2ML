{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Praktikum 2 IF 4071\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementasi Oleh Kelompok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class NeuralNet:\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, nodes_n_in_hidden_layers, learning_rate, momentum):\n",
    "        nodes_n_in_hidden_layers.append(1) # satu node buat output\n",
    "        self.nodes_n_in_hidden_layers = nodes_n_in_hidden_layers\n",
    "        self.inputs = [] #data input\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.outputs = [] # output dari setiap node pada satu iterasi\n",
    "        self.weights = [] # weight dari setiap edge pada satu iterasi\n",
    "        self.biases = [] # bias dari setiap node pada satu iterasi\n",
    "        self.weight_biases = []\n",
    "        self.local_gradients = [] # local gradient dari setiap node pada satu iterasi\n",
    "        self.delta_weights = [] # delta weight dari setiap edge pada satu iterasi\n",
    "        self.delta_biases = [] # delta bias dari setiap node pada satu iterasi\n",
    "        self.layer_nodes = [] # node-node pada layer-layer\n",
    "        self.v = [] # v pada setiap node\n",
    "        self.targets = [] # target dari data input\n",
    "        self.test_outputs = [] # hasil predict data test\n",
    "        self.test_labels = [] # label hasil predict data test\n",
    "        \n",
    "    # Feed Forward\n",
    "    def feed_forward(self, datum_idx):\n",
    "        for i in range (1, len(self.layer_nodes)):\n",
    "            for j in range (0, len(self.layer_nodes[i])):\n",
    "                current_node = self.layer_nodes[i][j]\n",
    "                weights = []\n",
    "                weights.append(self.biases[current_node])\n",
    "                inputs = []\n",
    "                inputs.append(1)\n",
    "                for k in range(0, len(self.layer_nodes[i-1])):\n",
    "                    if (self.weights[self.layer_nodes[i-1][k]][current_node] != None):\n",
    "                            weights.append(self.weights[self.layer_nodes[i-1][k]][current_node])\n",
    "                    \n",
    "                    if (i==1):\n",
    "                        inputs.append(self.inputs[datum_idx][self.layer_nodes[i-1][k]])\n",
    "                    else:\n",
    "                        inputs.append(self.outputs[self.layer_nodes[i-1][k]])\n",
    "                v = np.dot(inputs, weights)\n",
    "                self.v[current_node] = v\n",
    "                self.outputs[current_node] = sigmoid(v)\n",
    "                \n",
    "    # Back Propagation\n",
    "    def back_propagation(self, datum_idx):\n",
    "        for i in range(len(self.layer_nodes)-1, 0, -1):\n",
    "            for j in range(len(self.layer_nodes[i])-1, -1, -1):\n",
    "                current_node = self.layer_nodes[i][j]\n",
    "                if (i == len(self.layer_nodes)-1):\n",
    "                    v = self.v[current_node]\n",
    "                    sig_v = sigmoid(v)\n",
    "                    self.local_gradients[current_node] = self.local_gradients[current_node] + (sig_v * (1 - sig_v) * (self.targets[datum_idx] - self.outputs[current_node]))\n",
    "                else:\n",
    "                    v = self.v[current_node]\n",
    "                    sig_v = sigmoid(v)\n",
    "                    weight_delta = 1\n",
    "                    for k in range(0, len(self.layer_nodes[i+1])):\n",
    "                        weight_delta = weight_delta * self.local_gradients[self.layer_nodes[i+1][k]] * self.weights[current_node][self.layer_nodes[i+1][k]]\n",
    "                    self.local_gradients[current_node] = self.local_gradients[current_node] + (sig_v * (1 - sig_v) * weight_delta)            \n",
    "                \n",
    "    # Update Weight\n",
    "    def update_weight(self):\n",
    "        for i in range(0, len(self.layer_nodes)-1):\n",
    "            for j in range(0, len(self.layer_nodes[i])):\n",
    "                current_node = self.layer_nodes[i][j]\n",
    "                for k in range(0, len(self.layer_nodes[i+1])):\n",
    "                    current_next_node = self.layer_nodes[i+1][k]\n",
    "                    self.delta_weights[current_node][current_next_node] = self.momentum * self.delta_weights[current_node][current_next_node] + self.learning_rate * self.local_gradients[current_next_node] * self.outputs[current_next_node]\n",
    "                    self.weights[current_node][current_next_node] += self.delta_weights[current_node][current_next_node]\n",
    "        for i in range(1, len(self.biases)):\n",
    "            new_bias = self.biases[i] + self.momentum  * self.delta_biases[i] + self.learning_rate * self.local_gradients[i]\n",
    "            self.delta_biases[i] = new_bias - self.biases[i]\n",
    "            self.biases[i] = new_bias\n",
    "            \n",
    "    # Fit\n",
    "    def fit(self, X, Y, batch_size, max_iter, threshold): # data = array of arrays\n",
    "        #data[0] ke n merupakan label\n",
    "        #nodes_n_in_hidden_layers[0] merupakan jumlah input\n",
    "        self.nodes_n_in_hidden_layers.insert(0, len(X[0])) \n",
    "        \n",
    "        self.targets = Y\n",
    "        self.inputs = X\n",
    "        \n",
    "        n_nodes = 0\n",
    "        init_weight = 1 # Weights diinisalisasi 0\n",
    "        \n",
    "        # Inisialisasi output, bias, local gradient, v, dan delta bias di setiap node pada layer\n",
    "        for i in range(0, len(self.nodes_n_in_hidden_layers)):\n",
    "            l_nodes = []\n",
    "            for j in range(0, self.nodes_n_in_hidden_layers[i]):\n",
    "                self.outputs.append(0)\n",
    "                self.v.append(0)\n",
    "                self.biases.append(0) #asumsi x bias = 1\n",
    "                self.local_gradients.append(0)\n",
    "                self.delta_biases.append(0)\n",
    "                l_nodes.append(n_nodes)\n",
    "                n_nodes += 1\n",
    "            self.layer_nodes.append(l_nodes)\n",
    "        \n",
    "        for i in range(0, n_nodes):\n",
    "            self.weights.append([])\n",
    "            self.delta_weights.append([])\n",
    "            for j in range(0, n_nodes):\n",
    "                self.weights[i].append(None)\n",
    "                self.delta_weights[i].append(None)\n",
    "            \n",
    "        current_node = 0\n",
    "        for i in range(0, len(self.nodes_n_in_hidden_layers)-1):\n",
    "            if (i < len(self.nodes_n_in_hidden_layers)-1):\n",
    "                next_layer_first_node = current_node + self.nodes_n_in_hidden_layers[i]\n",
    "                for j in range(0, self.nodes_n_in_hidden_layers[i]):\n",
    "                    for k in range(0, self.nodes_n_in_hidden_layers[i+1]):\n",
    "                        self.weights[current_node][k+next_layer_first_node] = random.uniform(-0.5,0.5)\n",
    "                        self.delta_weights[current_node][k+next_layer_first_node] = 0\n",
    "                    current_node += 1\n",
    "        \n",
    "        n_batch = math.ceil(len(Y)/batch_size)\n",
    "        \n",
    "        n_iter = 0\n",
    "        error = 100\n",
    "        while (n_iter < max_iter and error > threshold):\n",
    "            datum_idx = 0\n",
    "            for i in range(0, n_batch):\n",
    "                # Mengembalikan local_gradient menjadi 0\n",
    "                for i in range(0, n_nodes):\n",
    "                    self.local_gradients[i] = 0\n",
    "                    \n",
    "                j = 0\n",
    "                accum_error = 0\n",
    "                num_datum = 0\n",
    "                while (j < batch_size):\n",
    "                    if (datum_idx < len(Y)):\n",
    "                        self.feed_forward(datum_idx)\n",
    "                        accum_error = accum_error + (0.5 * ((self.targets[datum_idx] - self.outputs[-1])**2))\n",
    "                        num_datum = num_datum + 1\n",
    "                        self.back_propagation(datum_idx)\n",
    "                        datum_idx += 1\n",
    "                        j += 1\n",
    "                    else:\n",
    "                        j = batch_size + 1\n",
    "                error = accum_error / num_datum\n",
    "                if (error < threshold):\n",
    "                    break\n",
    "                self.update_weight()\n",
    "            \n",
    "            n_iter += 1\n",
    "        \n",
    "    # Predict\n",
    "    def predict(self, data_test):\n",
    "        feed_forward_result = []\n",
    "        self.inputs = data_test\n",
    "        \n",
    "        for i in range (0, len(data_test)):\n",
    "            self.feed_forward(i)\n",
    "            self.test_outputs.append(self.outputs[-1])\n",
    "        \n",
    "        return self.test_outputs\n",
    "    \n",
    "    def predict_label(self):\n",
    "        for i in range (0, len(self.test_outputs)):\n",
    "            if self.test_outputs[i] >= 0.5:\n",
    "                self.test_labels.append(1)\n",
    "            else:\n",
    "                self.test_labels.append(0)\n",
    "        return self.test_labels\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementasi dengan Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers\n",
    "\n",
    "class KerasNeuralNet:\n",
    "    \n",
    "    # Construction\n",
    "    def __init__(self, nodes_n_in_hidden_layers, learning_rate, momentum):\n",
    "        nodes_n_in_hidden_layers.append(1) # satu node buat output\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(output_dim=nodes_n_in_hidden_layers[0], input_dim=4, activation=\"sigmoid\"))\n",
    "        for i in range(1, len(nodes_n_in_hidden_layers)):\n",
    "            self.model.add(Dense(output_dim=nodes_n_in_hidden_layers[i], input_dim=nodes_n_in_hidden_layers[i-1], activation='sigmoid'))\n",
    "        sgd = optimizers.SGD(lr=learning_rate, momentum=momentum, nesterov=False)\n",
    "        self.model.compile(loss='mean_squared_error', optimizer=sgd, metrics=['accuracy'])\n",
    "        \n",
    "    def fit(self, X, Y, batch_size, max_iter):\n",
    "        self.model.fit(X, Y, batch_size=batch_size, epochs=max_iter)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Membaca Data Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.06666667 0.13333333 0.2        0.26666667 0.33333333\n",
      " 0.4        0.46666667 0.53333333 0.6        0.66666667 0.73333333\n",
      " 0.8        0.86666667 0.93333333 1.        ]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "# Load data\n",
    "dataframe = pd.read_csv('data_training_praktikum.csv')\n",
    "dataframe = dataframe.drop(columns=['gap_formulaic', 'no_textstructure_formulaic','them_formulaic','us_previous_formulaic','affect',\\\n",
    "                       'argumentation','better_solution','change','comparison','continue','contrast','interest','need',\\\n",
    "                       'presentation','problem','research','solution','textstructure','use','copula','aim_ref_agent',\\\n",
    "                        'gap_agent','general_agent','ref_agent','ref_us_agent','textstructure_agent','them_agent',\\\n",
    "                       'them_pronoun_agent','us_agent'])\n",
    "\n",
    "# Transform outlook, temperature, humidity, and windy to numerical values\n",
    "enc = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "data_X = dataframe.values[:,0:30]\n",
    "data_Y = dataframe.values[:,30]\n",
    "data_X = enc.fit_transform(data_X).toarray()\n",
    "\n",
    "\n",
    "\n",
    "#reScale\n",
    "le = LabelEncoder()\n",
    "data_Y = le.fit_transform(data_Y)\n",
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "\n",
    "matrix_Y = []\n",
    "for i in range(0, len(data_Y)):\n",
    "    matrix_Y.append([data_Y[i]])\n",
    "data_Y = scaler.fit_transform(matrix_Y)\n",
    "\n",
    "array_Y = []\n",
    "for i in range(0, len(data_Y)):\n",
    "    array_Y.append(data_Y[i][0])\n",
    "data_Y = array_Y\n",
    "\n",
    "print(np.unique(data_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting dengan Hasil Implementasi Kelompok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6371934873427952, 0.6336026730531541, 0.6539361621846131, 0.6508008415326048, 0.6450460198663713, 0.6413939157872407, 0.6496933577094223, 0.6424431892105261, 0.6366263075766724, 0.6552220214899362, 0.6432353410188569, 0.6552559471539289, 0.6583535244641116, 0.6470597371112335]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNet([6], 0.25, 0.0001)\n",
    "nn.fit(rescaledX, Y, 1, 5, 0.01)\n",
    "nn.predict(rescaledX)\n",
    "print(nn.test_outputs)\n",
    "print(nn.predict_label())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting dengan Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suzaneringoringo/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=4, activation=\"sigmoid\", units=6)`\n",
      "  del sys.path[0]\n",
      "/home/suzaneringoringo/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=6, activation=\"sigmoid\", units=1)`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 0.2565 - acc: 0.5714\n",
      "Epoch 2/5\n",
      "14/14 [==============================] - 0s 930us/step - loss: 0.2412 - acc: 0.6429\n",
      "Epoch 3/5\n",
      "14/14 [==============================] - 0s 911us/step - loss: 0.2368 - acc: 0.6429\n",
      "Epoch 4/5\n",
      "14/14 [==============================] - 0s 970us/step - loss: 0.2324 - acc: 0.6429\n",
      "Epoch 5/5\n",
      "14/14 [==============================] - 0s 922us/step - loss: 0.2331 - acc: 0.6429\n",
      "[[ 0.63403666]\n",
      " [ 0.59249121]\n",
      " [ 0.68072295]\n",
      " [ 0.66368133]\n",
      " [ 0.66530591]\n",
      " [ 0.62578082]\n",
      " [ 0.64969957]\n",
      " [ 0.64091301]\n",
      " [ 0.64231777]\n",
      " [ 0.67312676]\n",
      " [ 0.61770022]\n",
      " [ 0.64808822]\n",
      " [ 0.69283575]\n",
      " [ 0.62445682]]\n",
      "[[ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]]\n"
     ]
    }
   ],
   "source": [
    "knn = KerasNeuralNet([6], 0.25, 0.0001)\n",
    "\n",
    "# Train model\n",
    "knn.fit(rescaledX, Y, 1, 5)\n",
    "\n",
    "# Predict\n",
    "labels = knn.predict(rescaledX)\n",
    "print(labels)\n",
    "for i in range(0, len(labels)):\n",
    "    if (labels[i] >= 0.5):\n",
    "        labels[i] = 1\n",
    "    else:\n",
    "        labels[i] = 0\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perbandingan Hasil Implementasi Kelompok dengan Keras\n",
    "Meskipun label yang dihasilkan sama persis, nilai sebenarnya dari feed_forward berbeda. Hal ini dapat terjadi karena nilai weights pada saat inisialisasi adalah random sehingga kemungkinan besar berbeda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Pembagian Tugas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Helena Suzane Graciella (13515032)\n",
    "   \n",
    "2. Lathifah Nurrahmah (13515046)\n",
    "    \n",
    "3. Aya Aurora Rimbamorani (13515098)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
